{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dppy.multivariate_jacobi_ope import MultivariateJacobiOPE\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh as largest_eigsh\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "import math\n",
    "import array\n",
    "from random import seed\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e934d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generation import generate_data_uniform\n",
    "from Data_generation import generate_data_beta\n",
    "from Data_generation import generate_data_mixture_Gaussian\n",
    "from Jacobi_parameter import fit_Jacobi_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for dividing the data into batches\n",
    "def get_batches(X,y,batch_size,row,sampleops,i):\n",
    "    if sampleops.name == 'iid':\n",
    "        idx = random.sample(range(row), batch_size)\n",
    "    if sampleops.name == 'dpp':\n",
    "        idx = sampleops.DPP_list[i]\n",
    "    idx = np.sort(idx)\n",
    "    X_new = X[idx,:]\n",
    "    y_new = y[idx]\n",
    "    return X_new, y_new, idx\n",
    "\n",
    "# Function for computing the gradient\n",
    "def get_gradient(X,y,theta,weight,loss_type,elambda):\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        loss = - y * (1 - 1 / (1 + np.exp(- hypothesis * y)))\n",
    "    gradient = np.dot(X.T, weight * loss) + elambda * theta\n",
    "    return gradient\n",
    "\n",
    "# Function for generating the alternative gradient\n",
    "def get_gradient_dppway2(X,y,theta,batch_size,loss_type,elambda,sampleops):\n",
    "    N, d = X.shape\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        loss = - y * (1 - 1 / (1 + np.exp(- hypothesis * y)))\n",
    "    Xsample = dpp.sample()\n",
    "    gradient = 0\n",
    "    for i in range(batch_size):\n",
    "        xsam = Xsample[i,:]\n",
    "        tmp = np.dot(np.ones((N,1)),np.reshape(xsam,(1,d))) - X\n",
    "        weight = np.reshape(sampleops.var.pdf(tmp), (N,1))\n",
    "        nablahat = np.dot(X.T, weight * loss) / N\n",
    "        gradient = gradient + nablahat / sampleops.dpp.K(xsam, eval_pointwise=False) / sampleops.dpp.eval_w(xsam)\n",
    "    gradient = gradient + elambda * theta\n",
    "    return gradient\n",
    "\n",
    "# Function for computing the function value\n",
    "def get_fun_value(X,y,theta,N,loss_type,elambda):\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "        fun_value = 0.5 * np.dot(loss.T,loss) / N + 0.5 * elambda * np.dot(theta.T,theta)\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        fun_value = np.sum(np.log(1 + np.exp(-hypothesis * y))) / N + 0.5 * elambda * np.dot(theta.T,theta)\n",
    "    return fun_value\n",
    "\n",
    "# Function for generating the DPP kernel for first way of gradient estimation\n",
    "def generate_DPP_kernel(X,N,p,dpp,gammatildeX):\n",
    "    Kq = dpp.K(X, eval_pointwise=False)\n",
    "    qX = dpp.eval_w(X)\n",
    "    D = np.diag(np.sqrt(np.divide(qX, gammatildeX)))\n",
    "    Ktilde = 1. / N * D @ Kq @ D\n",
    "    evals_large_sparse, evecs_large_sparse = largest_eigsh(Ktilde, p, which='LM')\n",
    "    evals_large_sparse = np.ones(p)\n",
    "    Ktilde = np.dot(evecs_large_sparse,evecs_large_sparse.T)\n",
    "    diagKtilde = np.diag(Ktilde)\n",
    "    return evals_large_sparse, evecs_large_sparse, diagKtilde\n",
    "\n",
    "# Function for sampling the finite DPP\n",
    "def generate_DPP_list_of_samples(eig_vals, eig_vecs, maxit):\n",
    "    DPP = FiniteDPP(kernel_type='correlation',projection=True,\n",
    "                    **{'K_eig_dec': (eig_vals, eig_vecs)})\n",
    "    for _ in range(maxit):\n",
    "        DPP.sample_exact(mode='GS')\n",
    "    return DPP.list_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd50aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch SGD\n",
    "def MiniBatchSGD(X,y,theta,loss_type,elambda,batch_size,maxiter,sampleops,thetastar=np.nan):\n",
    "    N, d = X.shape\n",
    "    if sampleops.name == 'iid':\n",
    "        weight = (1 / batch_size) * np.ones((N, 1))\n",
    "    if sampleops.name == 'dpp':\n",
    "        weight = np.reshape(1 / sampleops.kernel_diag / N, (N,1))\n",
    "    loss_total = np.array(maxiter*[[0.0]])\n",
    "    gradient_total = np.array(maxiter*[[0.0]])\n",
    "    if ~np.isnan(thetastar).all():\n",
    "        error = np.array(maxiter*[[0.0]])\n",
    "    for i in range(maxiter):\n",
    "        if sampleops.name == 'dpp2':\n",
    "            gradient = get_gradient_dppway2(X,y,theta,batch_size,loss_type,elambda,sampleops)\n",
    "        else:\n",
    "            X_batch, y_batch, idx = get_batches(X,y,batch_size,N,sampleops,i)\n",
    "            gradient = get_gradient(X_batch,y_batch,theta,weight[idx],loss_type,elambda)\n",
    "        theta = theta - (1/(i+1)**0.9) * gradient\n",
    "        loss_total[i] = get_fun_value(X,y,theta,N,loss_type,elambda)\n",
    "        gradient_total[i] = np.linalg.norm(get_gradient(X,y,theta,(1 / N) * np.ones((N, 1)),loss_type,elambda), 2)\n",
    "        if ~np.isnan(thetastar).all():\n",
    "            error[i] = np.linalg.norm(theta - thetastar, 2)\n",
    "    if np.isnan(thetastar).all():\n",
    "        return theta, loss_total, gradient_total\n",
    "    if ~np.isnan(thetastar).all():\n",
    "        return theta, loss_total, gradient_total, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ff2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_ops:\n",
    "    def __init__(self):\n",
    "        self.name = []\n",
    "\n",
    "N, d = 1000, 2\n",
    "losstype = 'linear_regression'\n",
    "\n",
    "#X, y = generate_data_uniform(N, d)\n",
    "X ,y = generate_data_mixture_Gaussian(N, d, 2)\n",
    "Z = np.concatenate((X,y),axis = 1)\n",
    "        \n",
    "theta0 = np.array(d*[[0.0]])\n",
    "lambda_input = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4290eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if losstype == 'linear_regression':\n",
    "    inv = np.linalg.inv(np.dot(X.T, X) + N * lambda_input * np.identity(d))\n",
    "    theta_direct = np.dot(inv, np.dot(X.T, y))\n",
    "if losstype == 'logistic_regression':\n",
    "    def objfun(x):\n",
    "        x = np.reshape(x,(d,1))\n",
    "        hypothesis = np.dot(X, x)\n",
    "        fun_value = np.sum(np.log(1 + np.exp(-hypothesis * y))) / N + 0.5 * lambda_input * np.dot(x.T,x)\n",
    "        fun_value = fun_value[0,0]\n",
    "        return fun_value\n",
    "    theta_solve = minimize(objfun, np.reshape(theta0,(d,)), tol=1e-16)\n",
    "    theta_direct = np.reshape(theta_solve.x,(d,1))\n",
    "gradient = get_gradient(X,y,theta_direct,(1 / N) * np.ones((N, 1)),losstype,lambda_input)\n",
    "norm_gradient = np.linalg.norm(gradient, 2)\n",
    "print('True solution obtained, with norm of gradient = ',norm_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb722dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = 5\n",
    "maxit2 = int(5*N/p2)\n",
    "p3 = 10\n",
    "maxit3 = int(5*N/p3)\n",
    "p4 = p2\n",
    "maxit4 = int(5*N/p4)\n",
    "p5 = p3\n",
    "maxit5 = int(5*N/p5)\n",
    "p6 = p2\n",
    "maxit6 = int(5*N/p6)\n",
    "p7 = p3\n",
    "maxit7 = int(5*N/p7)\n",
    "\n",
    "trial_total = 1000\n",
    "\n",
    "ops = sample_ops()\n",
    "ops.name = 'iid'\n",
    "\n",
    "## generate DPP kernel and gammatilde\n",
    "jac_params = fit_Jacobi_parameters(Z)\n",
    "#jac_params = np.array([[1. / 3, 1. / 2] for _ in range(d+1)])\n",
    "gammatilde = stats.gaussian_kde(Z.T)\n",
    "gammatilde.set_bandwidth(bw_method='silverman')\n",
    "gammatildeZ = gammatilde.evaluate(Z.T)\n",
    "#gammatildeZ = 1./2**(d+1) * np.ones((N,))\n",
    "    \n",
    "for trial_num in range(trial_total):\n",
    "    ops1 = sample_ops()\n",
    "    ops1.name = 'iid'\n",
    "    \n",
    "    theta_start, loss_total, grad_total, error = MiniBatchSGD(X, y, theta0, loss_type=losstype, elambda=lambda_input,\n",
    "              batch_size=N, maxiter=1, sampleops=ops, thetastar=theta_direct)\n",
    "    \n",
    "    theta2, loss_total2, grad_total2, error2 = MiniBatchSGD(X, y, theta_start, loss_type=losstype, elambda=lambda_input,\n",
    "                         batch_size=p2, maxiter=maxit2, sampleops=ops1, thetastar=theta_direct)\n",
    "    \n",
    "    theta3, loss_total3, grad_total3, error3 = MiniBatchSGD(X, y, theta_start, loss_type=losstype, elambda=lambda_input,\n",
    "                     batch_size=p3, maxiter=maxit3, sampleops=ops1, thetastar=theta_direct)\n",
    "\n",
    "    ops2 = sample_ops()\n",
    "    ops2.name = 'dpp'\n",
    "    \n",
    "    dpp = MultivariateJacobiOPE(p4, jac_params)\n",
    "    eig_vals, eig_vecs, diagKtilde = generate_DPP_kernel(Z,N,p4,dpp,gammatildeZ)\n",
    "    ops2.kernel_diag = diagKtilde\n",
    "    ops2.DPP_list = generate_DPP_list_of_samples(eig_vals, eig_vecs, maxit4)\n",
    "    theta4, loss_total4, grad_total4, error4 = MiniBatchSGD(X, y, theta_start, loss_type=losstype, elambda=lambda_input,\n",
    "                         batch_size=p4, maxiter=maxit4, sampleops=ops2, thetastar=theta_direct)\n",
    "\n",
    "    dpp = MultivariateJacobiOPE(p5, jac_params)\n",
    "    eig_vals, eig_vecs, diagKtilde = generate_DPP_kernel(Z,N,p5,dpp,gammatildeZ)\n",
    "    ops2.kernel_diag = diagKtilde\n",
    "    ops2.DPP_list = generate_DPP_list_of_samples(eig_vals, eig_vecs, maxit5)\n",
    "    theta5, loss_total5, grad_total5, error5 = MiniBatchSGD(X, y, theta_start, loss_type=losstype, elambda=lambda_input,\n",
    "                         batch_size=p5, maxiter=maxit5, sampleops=ops2, thetastar=theta_direct)\n",
    "    \n",
    "    if trial_num == 0:\n",
    "        loss2 = loss_total2\n",
    "        loss3 = loss_total3\n",
    "        loss4 = loss_total4\n",
    "        loss5 = loss_total5\n",
    "        grad2 = grad_total2\n",
    "        grad3 = grad_total3\n",
    "        grad4 = grad_total4\n",
    "        grad5 = grad_total5\n",
    "        err2 = error2\n",
    "        err3 = error3\n",
    "        err4 = error4\n",
    "        err5 = error5\n",
    "    else:\n",
    "        loss2 = np.hstack((loss2, loss_total2))\n",
    "        loss3 = np.hstack((loss3, loss_total3))\n",
    "        loss4 = np.hstack((loss4, loss_total4))\n",
    "        loss5 = np.hstack((loss5, loss_total5))\n",
    "        grad2 = np.hstack((grad2, grad_total2))\n",
    "        grad3 = np.hstack((grad3, grad_total3))\n",
    "        grad4 = np.hstack((grad4, grad_total4))\n",
    "        grad5 = np.hstack((grad5, grad_total5))\n",
    "        err2 = np.hstack((err2, error2))\n",
    "        err3 = np.hstack((err3, error3))\n",
    "        err4 = np.hstack((err4, error4))\n",
    "        err5 = np.hstack((err5, error5))\n",
    "    print(trial_num, 'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50416b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = int(0.1*N/p2)\n",
    "start3 = int(0.1*N/p3)\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plot1 = plt.figure(1)\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),loss2.mean(1)[start2:maxit2],'r', label='iid_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),loss3.mean(1)[start3:maxit3],'b', label='iid_p10')\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),loss4.mean(1)[start2:maxit2],'r--', label='dpp_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),loss5.mean(1)[start3:maxit3],'b--', label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('function value')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plot2 = plt.figure(2)\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),grad2.mean(1)[start2:maxit2],'r', label='iid_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),grad3.mean(1)[start3:maxit3],'b', label='iid_p10')\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),grad4.mean(1)[start2:maxit2],'r--', label='dpp_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),grad5.mean(1)[start3:maxit3],'b--', label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('norm of gradient')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plot3 = plt.figure(3)\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),err2.mean(1)[start2:maxit2],'r', label='iid_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),err3.mean(1)[start3:maxit3],'b', label='iid_p10')\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),err4.mean(1)[start2:maxit2],'r--', label='dpp_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),err5.mean(1)[start3:maxit3],'b--', label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('$||\\Theta_t-\\Theta_*||_2$')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "plot4 = plt.figure(4)\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),grad2.mean(1)[start2:maxit2],'r', label='iid_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),grad3.mean(1)[start3:maxit3],'b', label='iid_p10')\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),grad4.mean(1)[start2:maxit2],'r--', label='dpp_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),grad5.mean(1)[start3:maxit3],'b--', label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('norm of gradient')\n",
    "plt.legend(fontsize=20)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plot5 = plt.figure(5)\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),err2.mean(1)[start2:maxit2],'r', label='iid_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),err3.mean(1)[start3:maxit3],'b', label='iid_p10')\n",
    "plt.plot(range(start2*p2,maxit2*p2,p2),err4.mean(1)[start2:maxit2],'r--', label='dpp_p5')\n",
    "plt.plot(range(start3*p3,maxit3*p3,p3),err5.mean(1)[start3:maxit3],'b--', label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('$||\\Theta_t-\\Theta_*||_2$')\n",
    "plt.legend(fontsize=20)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eafd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = int(N/p2)\n",
    "start3 = int(0.25*N/p3)\n",
    "start4 = int(0.5*N/p4)\n",
    "start5 = int(0.75*N/p5)\n",
    "\n",
    "gap2 = int(N/p2)\n",
    "gap3 = int(N/p3)\n",
    "gap4 = int(N/p4)\n",
    "gap5 = int(N/p5)\n",
    "\n",
    "idx2 = range(start2,maxit2,gap2)\n",
    "idx2 = np.concatenate((np.array([int(0.25*N/p2)]),idx2))\n",
    "xidx2 = np.array(range(start2*p2,maxit2*p2,p2*gap2))\n",
    "xidx2 = np.concatenate((np.array([int(0.25*N)]),xidx2))\n",
    "yidx2 = grad2.mean(1)[idx2]\n",
    "zidx2 = err2.mean(1)[idx2]\n",
    "grad_std_idx2 = np.std(grad2, axis=1)[idx2]\n",
    "err_std_idx2 = np.std(err2, axis=1)[idx2]\n",
    "\n",
    "idx4 = range(start4,maxit4+start4-int(N/p4)+gap4,gap4)\n",
    "idx4 = np.concatenate((np.array([int(0.25*N/p4)]),idx4))\n",
    "xidx4 = np.array(range(start4*p4,maxit4*p4+start4*p4-N+p4*gap4,p4*gap4))\n",
    "xidx4 = np.concatenate((np.array([int(0.25*N)]),xidx4))\n",
    "yidx4 = grad4.mean(1)[idx4]\n",
    "zidx4 = err4.mean(1)[idx4]\n",
    "grad_std_idx4 = np.std(grad4, axis=1)[idx4]\n",
    "err_std_idx4 = np.std(err4, axis=1)[idx4]\n",
    "\n",
    "idx3 = range(start3,maxit3+start3-int(N/p3)+gap3,gap3)\n",
    "idx3 = np.concatenate((np.array([int(0.25*N/p3)]),idx3))\n",
    "xidx3 = np.array(range(start3*p3,maxit3*p3+start3*p3-N+p3*gap3,p3*gap3))\n",
    "xidx3 = np.concatenate((np.array([int(0.25*N)]),xidx3))\n",
    "yidx3 = grad3.mean(1)[idx3]\n",
    "zidx3 = err3.mean(1)[idx3]\n",
    "grad_std_idx3 = np.std(grad3, axis=1)[idx3]\n",
    "err_std_idx3 = np.std(err3, axis=1)[idx3]\n",
    "\n",
    "idx5 = range(start5,maxit5+start5-int(N/p5)+gap5,gap5)\n",
    "idx5 = np.concatenate((np.array([int(0.25*N/p5)]),idx5))\n",
    "xidx5 = np.array(range(start5*p5,maxit5*p5+start5*p5-N+p3*gap5,p3*gap5))\n",
    "xidx5 = np.concatenate((np.array([int(0.25*N)]),xidx5))\n",
    "yidx5 = grad5.mean(1)[idx5]\n",
    "zidx5 = err5.mean(1)[idx5]\n",
    "grad_std_idx5 = np.std(grad5, axis=1)[idx5]\n",
    "err_std_idx5 = np.std(err5, axis=1)[idx5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(xidx2, yidx2, yerr=grad_std_idx2/np.sqrt(trial_total), fmt='r-', label='iid_p5')\n",
    "ax.errorbar(xidx3, yidx3, yerr=grad_std_idx3/np.sqrt(trial_total), fmt='b-', label='iid_p10')\n",
    "ax.errorbar(xidx4, yidx4, yerr=grad_std_idx4/np.sqrt(trial_total), fmt='r--', label='dpp_p5')\n",
    "ax.errorbar(xidx5, yidx5, yerr=grad_std_idx5/np.sqrt(trial_total), fmt='b--', label='dpp_p10')\n",
    "ax.legend()\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('norm of gradient')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(xidx2, zidx2, yerr=err_std_idx2/np.sqrt(trial_total), fmt='r-', label='iid_p5')\n",
    "ax.errorbar(xidx3, zidx3, yerr=err_std_idx3/np.sqrt(trial_total), fmt='b-', label='iid_p10')\n",
    "ax.errorbar(xidx4, zidx4, yerr=err_std_idx4/np.sqrt(trial_total), fmt='r--', label='dpp_p5')\n",
    "ax.errorbar(xidx5, zidx5, yerr=err_std_idx5/np.sqrt(trial_total), fmt='b--', label='dpp_p10')\n",
    "ax.legend()\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('$||\\Theta_t-\\Theta_*||_2$')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
