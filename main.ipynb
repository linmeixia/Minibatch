{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dppy.multivariate_jacobi_ope import MultivariateJacobiOPE\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh as largest_eigsh\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "import math\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e934d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generation import generate_data_uniform\n",
    "from Data_generation import generate_data_beta\n",
    "from Data_generation import generate_data_mixture_Gaussian\n",
    "from Jacobi_parameter import fit_Jacobi_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for dividing the data into batches\n",
    "def get_batches(X,y,batch_size,row,sampleops,i):\n",
    "    if sampleops.name == 'iid':\n",
    "        idx = random.sample(range(row), batch_size)\n",
    "    if sampleops.name == 'dpp':\n",
    "        idx = sampleops.DPP_list[i]\n",
    "    idx = np.sort(idx)\n",
    "    X_new = X[idx,:]\n",
    "    y_new = y[idx]\n",
    "    return X_new, y_new, idx\n",
    "\n",
    "# Function for computing the gradient\n",
    "def get_gradient(X,y,theta,weight,loss_type,elambda):\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        loss = - y * (1 - 1 / (1 + np.exp(- hypothesis * y)))\n",
    "    gradient = np.dot(X.T, weight * loss) + elambda * theta\n",
    "    return gradient\n",
    "\n",
    "# Function for generating the alternative gradient\n",
    "def get_gradient_dppway2(X,y,theta,batch_size,loss_type,elambda,sampleops):\n",
    "    N, d = X.shape\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        loss = - y * (1 - 1 / (1 + np.exp(- hypothesis * y)))\n",
    "    Xsample = dpp.sample()\n",
    "    gradient = 0\n",
    "    for i in range(batch_size):\n",
    "        xsam = Xsample[i,:]\n",
    "        tmp = np.dot(np.ones((N,1)),np.reshape(xsam,(1,d))) - X\n",
    "        weight = np.reshape(sampleops.var.pdf(tmp), (N,1))\n",
    "        nablahat = np.dot(X.T, weight * loss) / N\n",
    "        gradient = gradient + nablahat / sampleops.dpp.K(xsam, eval_pointwise=False) / sampleops.dpp.eval_w(xsam)\n",
    "    gradient = gradient + elambda * theta\n",
    "    return gradient\n",
    "\n",
    "# Function for computing the function value\n",
    "def get_fun_value(X,y,theta,N,loss_type,elambda):\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "        fun_value = 0.5 * np.dot(loss.T,loss) / N + 0.5 * elambda * np.dot(theta.T,theta)\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        fun_value = np.sum(np.log(1 + np.exp(-hypothesis * y))) / N + 0.5 * elambda * np.dot(theta.T,theta)\n",
    "    return fun_value\n",
    "\n",
    "# Function for generating the DPP kernel for first way of gradient estimation\n",
    "def generate_DPP_kernel(X,N,p,dpp,gammatildeX):\n",
    "    Kq = dpp.K(X, eval_pointwise=False)\n",
    "    qX = dpp.eval_w(X)\n",
    "    D = np.diag(np.sqrt(np.divide(qX, gammatildeX)))\n",
    "    Ktilde = 1. / N * D @ Kq @ D\n",
    "    evals_large_sparse, evecs_large_sparse = largest_eigsh(Ktilde, p, which='LM')\n",
    "    evals_large_sparse = np.ones(p)\n",
    "    Ktilde = np.dot(evecs_large_sparse,evecs_large_sparse.T)\n",
    "    diagKtilde = np.diag(Ktilde)\n",
    "    return evals_large_sparse, evecs_large_sparse, diagKtilde\n",
    "\n",
    "# Function for sampling the finite DPP\n",
    "def generate_DPP_list_of_samples(eig_vals, eig_vecs, maxit):\n",
    "    DPP = FiniteDPP(kernel_type='correlation',projection=True,\n",
    "                    **{'K_eig_dec': (eig_vals, eig_vecs)})\n",
    "    for _ in range(maxit):\n",
    "        DPP.sample_exact(mode='GS')\n",
    "    return DPP.list_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd50aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch SGD\n",
    "def MiniBatchSGD(X,y,theta,loss_type,elambda,batch_size,maxiter,sampleops,thetastar=np.nan):\n",
    "    N, d = X.shape\n",
    "    if sampleops.name == 'iid':\n",
    "        weight = (1 / batch_size) * np.ones((N, 1))\n",
    "    if sampleops.name == 'dpp':\n",
    "        weight = np.reshape(1 / sampleops.kernel_diag / N, (N,1))\n",
    "    loss_total = np.array(maxiter*[[0.0]])\n",
    "    gradient_total = np.array(maxiter*[[0.0]])\n",
    "    if ~np.isnan(thetastar).all():\n",
    "        error = np.array(maxiter*[[0.0]])\n",
    "    for i in range(maxiter):\n",
    "        if sampleops.name == 'dpp2':\n",
    "            gradient = get_gradient_dppway2(X,y,theta,batch_size,loss_type,elambda,sampleops)\n",
    "        else:\n",
    "            X_batch, y_batch, idx = get_batches(X,y,batch_size,N,sampleops,i)\n",
    "            gradient = get_gradient(X_batch,y_batch,theta,weight[idx],loss_type,elambda)\n",
    "        theta = theta - (1/(i+1)**0.9) * gradient\n",
    "        loss_total[i] = get_fun_value(X,y,theta,N,loss_type,elambda)\n",
    "        gradient_total[i] = np.linalg.norm(get_gradient(X,y,theta,(1 / N) * np.ones((N, 1)),loss_type,elambda), 2)\n",
    "        if ~np.isnan(thetastar).all():\n",
    "            error[i] = np.linalg.norm(theta - thetastar, 2)\n",
    "    if np.isnan(thetastar).all():\n",
    "        return theta, loss_total, gradient_total\n",
    "    if ~np.isnan(thetastar).all():\n",
    "        return theta, loss_total, gradient_total, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ff2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_ops:\n",
    "    def __init__(self):\n",
    "        self.name = []\n",
    "\n",
    "N, d = 1000, 2\n",
    "losstype = 'linear_regression'\n",
    "\n",
    "X, y = generate_data_uniform(N, d)\n",
    "# X ,y = generate_data_mixture_Gaussian(N, d, 4)\n",
    "if losstype == 'logistic_regression':\n",
    "    y = np.sign(y)\n",
    "    tmp = np.argwhere(y == 0)\n",
    "    if tmp.size > 0:\n",
    "        y[tmp] = 1\n",
    "if losstype == 'linear_regression':\n",
    "    Z = np.concatenate((X,y),axis = 1)\n",
    "    dcom = d + 1\n",
    "elif losstype == 'logistic_regression':\n",
    "    Z = X\n",
    "    dcom = d\n",
    "    \n",
    "theta0 = np.array(d*[[0.0]])\n",
    "lambda_input = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if losstype == 'linear_regression':\n",
    "    inv = np.linalg.inv(np.dot(X.T, X) + N * lambda_input * np.identity(d))\n",
    "    theta_direct = np.dot(inv, np.dot(X.T, y))\n",
    "if losstype == 'logistic_regression':\n",
    "    def objfun(x):\n",
    "        x = np.reshape(x,(d,1))\n",
    "        hypothesis = np.dot(X, x)\n",
    "        fun_value = np.sum(np.log(1 + np.exp(-hypothesis * y))) / N + 0.5 * lambda_input * np.dot(x.T,x)\n",
    "        fun_value = fun_value[0,0]\n",
    "        return fun_value\n",
    "    theta_solve = minimize(objfun, np.reshape(theta0,(d,)), tol=1e-16)\n",
    "    theta_direct = np.reshape(theta_solve.x,(d,1))\n",
    "gradient = get_gradient(X,y,theta_direct,(1 / N) * np.ones((N, 1)),losstype,lambda_input)\n",
    "norm_gradient = np.linalg.norm(gradient, 2)\n",
    "print('True solution obtained, with norm of gradient = ',norm_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate DPP kernel and gammatilde\n",
    "jac_params = fit_Jacobi_parameters(Z)\n",
    "gammatilde = stats.gaussian_kde(Z.T)\n",
    "gammatilde.set_bandwidth(bw_method='silverman')\n",
    "gammatildeZ = gammatilde.evaluate(Z.T)\n",
    "\n",
    "p4 = 10\n",
    "maxit4 = int(2 * N / p4)\n",
    "ops2 = sample_ops()\n",
    "ops2.name = 'dpp'\n",
    "dpp = MultivariateJacobiOPE(p4, jac_params)\n",
    "eig_vals, eig_vecs, diagKtilde = generate_DPP_kernel(Z,N,p4,dpp,gammatildeZ)\n",
    "ops2.kernel_diag = diagKtilde\n",
    "ops2.DPP_list = generate_DPP_list_of_samples(eig_vals, eig_vecs, maxit4)\n",
    "\n",
    "theta4, loss_total4, grad_total4, error4 = MiniBatchSGD(X, y, theta0, loss_type=losstype, elambda=lambda_input,\n",
    "                     batch_size=p4, maxiter=maxit4, sampleops=ops2, thetastar=theta_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c53942",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "plot1 = plt.figure(3)\n",
    "plt.plot(range(0,maxit4*p4,p4),loss_total4, label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('function value')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plot2 = plt.figure(4)\n",
    "plt.plot(range(0,maxit4*p4,p4),grad_total4, label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('norm of gradient')\n",
    "plot2.suptitle('Norm of gradient v.s. budget')\n",
    "plt.legend(fontsize=20)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plot3 = plt.figure(5)\n",
    "plt.plot(range(0,maxit4*p4,p4),error4, label='dpp_p10')\n",
    "plt.xlabel('budget')\n",
    "plt.ylabel('$||\\Theta_t-\\Theta_*||_2$')\n",
    "plt.legend(fontsize=20)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
