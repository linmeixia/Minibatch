{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dppy.multivariate_jacobi_ope import MultivariateJacobiOPE\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh as largest_eigsh\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "import math\n",
    "import array\n",
    "from random import seed\n",
    "from random import randint\n",
    "from scipy.stats.distributions import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e934d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generation import generate_data_uniform\n",
    "from Data_generation import generate_data_beta\n",
    "from Data_generation import generate_data_mixture_Gaussian\n",
    "from Jacobi_parameter import fit_Jacobi_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d084c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for dividing the data into batches\n",
    "def get_batches(X,y,batch_size,row,sampleops,i):\n",
    "    if sampleops.name == 'iid':\n",
    "        idx = random.sample(range(row), batch_size)\n",
    "    if sampleops.name == 'dpp':\n",
    "        idx = sampleops.DPP_list[i]\n",
    "    idx = np.sort(idx)\n",
    "    X_new = X[idx,:]\n",
    "    y_new = y[idx]\n",
    "    return X_new, y_new, idx\n",
    "\n",
    "# Function for computing the gradient\n",
    "def get_gradient(X,y,theta,weight,loss_type,elambda):\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        loss = - y * (1 - 1 / (1 + np.exp(- hypothesis * y)))\n",
    "    gradient = np.dot(X.T, weight * loss) + elambda * theta\n",
    "    return gradient\n",
    "\n",
    "# Function for generating the alternative gradient\n",
    "def get_gradient_dppway2(X,y,theta,batch_size,loss_type,elambda,sampleops):\n",
    "    N, d = X.shape\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        loss = - y * (1 - 1 / (1 + np.exp(- hypothesis * y)))\n",
    "    Xsample = dpp.sample()\n",
    "    gradient = 0\n",
    "    for i in range(batch_size):\n",
    "        xsam = Xsample[i,:]\n",
    "        tmp = np.dot(np.ones((N,1)),np.reshape(xsam,(1,d))) - X\n",
    "        weight = np.reshape(sampleops.var.pdf(tmp), (N,1))\n",
    "        nablahat = np.dot(X.T, weight * loss) / N\n",
    "        gradient = gradient + nablahat / sampleops.dpp.K(xsam, eval_pointwise=False) / sampleops.dpp.eval_w(xsam)\n",
    "    gradient = gradient + elambda * theta\n",
    "    return gradient\n",
    "\n",
    "# Function for computing the function value\n",
    "def get_fun_value(X,y,theta,N,loss_type,elambda):\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    if loss_type == 'linear_regression':\n",
    "        loss = hypothesis - y\n",
    "        fun_value = 0.5 * np.dot(loss.T,loss) / N + 0.5 * elambda * np.dot(theta.T,theta)\n",
    "    elif loss_type == 'logistic_regression':\n",
    "        fun_value = np.sum(np.log(1 + np.exp(-hypothesis * y))) / N + 0.5 * elambda * np.dot(theta.T,theta)\n",
    "    return fun_value\n",
    "\n",
    "# Function for generating the DPP kernel for first way of gradient estimation\n",
    "def generate_DPP_kernel(X,N,p,dpp,gammatildeX):\n",
    "    Kq = dpp.K(X, eval_pointwise=False)\n",
    "    qX = dpp.eval_w(X)\n",
    "    D = np.diag(np.sqrt(np.divide(qX, gammatildeX)))\n",
    "    Ktilde = 1. / N * D @ Kq @ D\n",
    "    evals_large_sparse, evecs_large_sparse = largest_eigsh(Ktilde, p, which='LM')\n",
    "    evals_large_sparse = np.ones(p)\n",
    "    Ktilde = np.dot(evecs_large_sparse,evecs_large_sparse.T)\n",
    "    diagKtilde = np.diag(Ktilde)\n",
    "    return evals_large_sparse, evecs_large_sparse, diagKtilde\n",
    "\n",
    "# Function for sampling the finite DPP\n",
    "def generate_DPP_list_of_samples(eig_vals, eig_vecs, maxit):\n",
    "    DPP = FiniteDPP(kernel_type='correlation',projection=True,\n",
    "                    **{'K_eig_dec': (eig_vals, eig_vecs)})\n",
    "    for _ in range(maxit):\n",
    "        DPP.sample_exact(mode='GS')\n",
    "    return DPP.list_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e816c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_ops:\n",
    "    def __init__(self):\n",
    "        self.name = []\n",
    "losstype = 'linear_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, d = 1000, 1\n",
    "X, y = generate_data_uniform(N, d)\n",
    "y = np.ones((N,1))\n",
    "Z = X\n",
    "\n",
    "lambda_input = 0\n",
    "\n",
    "inv = np.linalg.inv(np.dot(X.T, X) + N * lambda_input * np.identity(d))\n",
    "theta_direct = np.dot(inv, np.dot(X.T, y))\n",
    "gradient = get_gradient(X,y,theta_direct,(1 / N) * np.ones((N, 1)),losstype,lambda_input)\n",
    "norm_gradient = np.linalg.norm(gradient, 2)\n",
    "print('True solution obtained, with norm of gradient = ',norm_gradient)\n",
    "\n",
    "jac_params = fit_Jacobi_parameters(Z)\n",
    "gammatilde = stats.gaussian_kde(Z.T)\n",
    "gammatilde.set_bandwidth(bw_method='silverman')\n",
    "gammatildeZ = gammatilde.evaluate(Z.T)\n",
    "\n",
    "theta = theta_direct\n",
    "batch_size_list = np.array([5,10,15,20,25,30,35,40])\n",
    "compute_grad_num = 1000\n",
    "\n",
    "grad_var_iid = np.array(len(batch_size_list)*[[0.0]])\n",
    "grad_var_dpp = np.array(len(batch_size_list)*[[0.0]])\n",
    "for k in range(len(batch_size_list)):\n",
    "    batch_size = int(batch_size_list[k])\n",
    "\n",
    "    dpp = MultivariateJacobiOPE(batch_size, jac_params)\n",
    "    eig_vals, eig_vecs, diagKtilde = generate_DPP_kernel(Z,N,batch_size,dpp,gammatildeZ)\n",
    "\n",
    "    weight_iid = (1 / batch_size) * np.ones((N, 1))\n",
    "    weight_dpp = np.reshape(1 / diagKtilde / N, (N,1))\n",
    "\n",
    "    for j in range(compute_grad_num):\n",
    "        idx = np.sort(random.sample(range(N), batch_size))\n",
    "        grad_iid_tmp = get_gradient(X[idx,:],y[idx],theta,weight_iid[idx],losstype,lambda_input)\n",
    "        if j == 0:\n",
    "            grad_iid = grad_iid_tmp\n",
    "        else:\n",
    "            grad_iid = np.hstack((grad_iid, grad_iid_tmp)) \n",
    "    grad_var_iid[k] = np.var(grad_iid, ddof=1)\n",
    "\n",
    "    DPP_list = generate_DPP_list_of_samples(eig_vals, eig_vecs, compute_grad_num)\n",
    "    for j in range(compute_grad_num):\n",
    "        idx = np.sort(DPP_list[j])\n",
    "        grad_dpp_tmp = get_gradient(X[idx,:],y[idx],theta,weight_dpp[idx],losstype,lambda_input)\n",
    "        if j == 0:\n",
    "            grad_dpp = grad_dpp_tmp\n",
    "        else:\n",
    "            grad_dpp = np.hstack((grad_dpp, grad_dpp_tmp)) \n",
    "    grad_var_dpp[k] = np.var(grad_dpp, ddof=1)\n",
    "    print(batch_size,' finished')\n",
    "    \n",
    "A = chi2.ppf(0.05/2, df=compute_grad_num-1)\n",
    "B = chi2.ppf(1-0.05/2, df=compute_grad_num-1)\n",
    "var_high = (compute_grad_num-1) / A * grad_var_dpp\n",
    "var_low = (compute_grad_num-1) / B * grad_var_dpp\n",
    "xmle = np.reshape(np.log(batch_size_list),(len(batch_size_list),1)) \n",
    "ymle = np.reshape(np.log(grad_var_dpp),(len(batch_size_list),1)) \n",
    "xmle = np.hstack((xmle,np.ones((len(batch_size_list),1))))\n",
    "mle_para = np.reshape(np.dot(np.linalg.inv(np.dot(xmle.T, xmle)), np.dot(xmle.T, ymle)),(2,))\n",
    "mle_y = np.exp(mle_para[0]*np.log(batch_size_list)+mle_para[1])\n",
    "mle_slope = mle_para[0]\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots() \n",
    "plt.loglog(batch_size_list,mle_y,'g-',label='MLE slope=%.2f' %mle_slope)\n",
    "plt.loglog(batch_size_list,grad_var_iid, 'o', markeredgecolor='k', markerfacecolor='w', markersize=5)\n",
    "plt.loglog(batch_size_list,grad_var_dpp,'ko', markersize=3)\n",
    "plt.loglog(batch_size_list,var_high,'ro', markersize=2)\n",
    "plt.loglog(batch_size_list,var_low,'bo', markersize=2)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('variance')\n",
    "ax.set_xticks( [5,50] )\n",
    "plt.xticks( ticks = [10,40], labels = ['$10^{1}$' , r'$4 \\times 10^{1}$'] )\n",
    "plt.legend(fontsize=18,loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, d = 1000, 1\n",
    "X, y = generate_data_uniform(N, d)\n",
    "# X ,y = generate_data_mixture_Gaussian(N, d, 4)\n",
    "y = y + np.random.normal(0.1, 0.5, size=(N, 1))\n",
    "y = np.minimum(np.maximum(y,-0.95),0.95)\n",
    "Z = np.concatenate((X,y),axis = 1)\n",
    "\n",
    "lambda_input = 0\n",
    "\n",
    "inv = np.linalg.inv(np.dot(X.T, X) + N * lambda_input * np.identity(d))\n",
    "theta_direct = np.dot(inv, np.dot(X.T, y))\n",
    "gradient = get_gradient(X,y,theta_direct,(1 / N) * np.ones((N, 1)),losstype,lambda_input)\n",
    "norm_gradient = np.linalg.norm(gradient, 2)\n",
    "print('True solution obtained, with norm of gradient = ',norm_gradient)\n",
    "\n",
    "jac_params = fit_Jacobi_parameters(Z)\n",
    "gammatilde = stats.gaussian_kde(Z.T)\n",
    "gammatilde.set_bandwidth(bw_method='silverman')\n",
    "gammatildeZ = gammatilde.evaluate(Z.T)\n",
    "\n",
    "theta = theta_direct\n",
    "batch_size_list = np.array([10,20,30,40,50,60,70,80,90,100])\n",
    "compute_grad_num = 1000\n",
    "\n",
    "grad_var_iid = np.array(len(batch_size_list)*[[0.0]])\n",
    "grad_var_dpp = np.array(len(batch_size_list)*[[0.0]])\n",
    "for k in range(len(batch_size_list)):\n",
    "    batch_size = int(batch_size_list[k])\n",
    "\n",
    "    dpp = MultivariateJacobiOPE(batch_size, jac_params)\n",
    "    eig_vals, eig_vecs, diagKtilde = generate_DPP_kernel(Z,N,batch_size,dpp,gammatildeZ)\n",
    "\n",
    "    weight_iid = (1 / batch_size) * np.ones((N, 1))\n",
    "    weight_dpp = np.reshape(1 / diagKtilde / N, (N,1))\n",
    "\n",
    "    for j in range(compute_grad_num):\n",
    "        idx = np.sort(random.sample(range(N), batch_size))\n",
    "        grad_iid_tmp = get_gradient(X[idx,:],y[idx],theta,weight_iid[idx],losstype,lambda_input)\n",
    "        if j == 0:\n",
    "            grad_iid = grad_iid_tmp\n",
    "        else:\n",
    "            grad_iid = np.hstack((grad_iid, grad_iid_tmp)) \n",
    "    grad_var_iid[k] = np.var(grad_iid, ddof=1)\n",
    "\n",
    "    DPP_list = generate_DPP_list_of_samples(eig_vals, eig_vecs, compute_grad_num)\n",
    "    for j in range(compute_grad_num):\n",
    "        idx = np.sort(DPP_list[j])\n",
    "        grad_dpp_tmp = get_gradient(X[idx,:],y[idx],theta,weight_dpp[idx],losstype,lambda_input)\n",
    "        if j == 0:\n",
    "            grad_dpp = grad_dpp_tmp\n",
    "        else:\n",
    "            grad_dpp = np.hstack((grad_dpp, grad_dpp_tmp)) \n",
    "    grad_var_dpp[k] = np.var(grad_dpp, ddof=1)\n",
    "    print(batch_size,' finished')\n",
    "\n",
    "A = chi2.ppf(0.05/2, df=compute_grad_num-1)\n",
    "B = chi2.ppf(1-0.05/2, df=compute_grad_num-1)\n",
    "var_high = (compute_grad_num-1) / A * grad_var_dpp\n",
    "var_low = (compute_grad_num-1) / B * grad_var_dpp\n",
    "xmle = np.reshape(np.log(batch_size_list),(len(batch_size_list),1)) \n",
    "ymle = np.reshape(np.log(grad_var_dpp),(len(batch_size_list),1)) \n",
    "xmle = np.hstack((xmle,np.ones((len(batch_size_list),1))))\n",
    "mle_para = np.reshape(np.dot(np.linalg.inv(np.dot(xmle.T, xmle)), np.dot(xmle.T, ymle)),(2,))\n",
    "mle_y = np.exp(mle_para[0]*np.log(batch_size_list)+mle_para[1])\n",
    "mle_slope = mle_para[0]\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots() \n",
    "plt.loglog(batch_size_list,mle_y,'g-',label='MLE slope=%.2f' %mle_slope)\n",
    "plt.loglog(batch_size_list,grad_var_iid, 'o', markeredgecolor='k', markerfacecolor='w', markersize=5)\n",
    "plt.loglog(batch_size_list,grad_var_dpp,'ko', markersize=3)\n",
    "plt.loglog(batch_size_list,var_high,'ro', markersize=2)\n",
    "plt.loglog(batch_size_list,var_low,'bo', markersize=2)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('variance')\n",
    "plt.legend(fontsize=18,loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3668b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, d = 1000, 2\n",
    "X, y = generate_data_uniform(N, d)\n",
    "# X ,y = generate_data_mixture_Gaussian(N, d, 4)\n",
    "y = y + np.random.normal(0.1, 0.5, size=(N, 1))\n",
    "y = np.minimum(np.maximum(y,-0.95),0.95)\n",
    "Z = np.concatenate((X,y),axis = 1)\n",
    "\n",
    "lambda_input = 0\n",
    "\n",
    "inv = np.linalg.inv(np.dot(X.T, X) + N * lambda_input * np.identity(d))\n",
    "theta_direct = np.dot(inv, np.dot(X.T, y))\n",
    "gradient = get_gradient(X,y,theta_direct,(1 / N) * np.ones((N, 1)),losstype,lambda_input)\n",
    "norm_gradient = np.linalg.norm(gradient, 2)\n",
    "print('True solution obtained, with norm of gradient = ',norm_gradient)\n",
    "\n",
    "jac_params = fit_Jacobi_parameters(Z)\n",
    "gammatilde = stats.gaussian_kde(Z.T)\n",
    "gammatilde.set_bandwidth(bw_method='silverman')\n",
    "gammatildeZ = gammatilde.evaluate(Z.T)\n",
    "\n",
    "theta = theta_direct\n",
    "batch_size_list = np.array([10,20,30,40,50,60,70,80,90,100])\n",
    "compute_grad_num = 1000\n",
    "\n",
    "grad_var_iid = np.array(len(batch_size_list)*[[0.0]])\n",
    "grad_var_dpp = np.array(len(batch_size_list)*[[0.0]])\n",
    "for k in range(len(batch_size_list)):\n",
    "    batch_size = int(batch_size_list[k])\n",
    "\n",
    "    dpp = MultivariateJacobiOPE(batch_size, jac_params)\n",
    "    eig_vals, eig_vecs, diagKtilde = generate_DPP_kernel(Z,N,batch_size,dpp,gammatildeZ)\n",
    "\n",
    "    weight_iid = (1 / batch_size) * np.ones((N, 1))\n",
    "    weight_dpp = np.reshape(1 / diagKtilde / N, (N,1))\n",
    "\n",
    "    for j in range(compute_grad_num):\n",
    "        idx = np.sort(random.sample(range(N), batch_size))\n",
    "        grad_iid_tmp = get_gradient(X[idx,:],y[idx],theta,weight_iid[idx],losstype,lambda_input)\n",
    "        if j == 0:\n",
    "            grad_iid = grad_iid_tmp\n",
    "        else:\n",
    "            grad_iid = np.hstack((grad_iid, grad_iid_tmp)) \n",
    "    grad_var_iid[k] = np.trace(np.cov(grad_iid, ddof=1))\n",
    "\n",
    "    DPP_list = generate_DPP_list_of_samples(eig_vals, eig_vecs, compute_grad_num)\n",
    "    for j in range(compute_grad_num):\n",
    "        idx = np.sort(DPP_list[j])\n",
    "        grad_dpp_tmp = get_gradient(X[idx,:],y[idx],theta,weight_dpp[idx],losstype,lambda_input)\n",
    "        if j == 0:\n",
    "            grad_dpp = grad_dpp_tmp\n",
    "        else:\n",
    "            grad_dpp = np.hstack((grad_dpp, grad_dpp_tmp)) \n",
    "    grad_var_dpp[k] = np.trace(np.cov(grad_dpp, ddof=1))\n",
    "    print(batch_size,' finished')\n",
    "\n",
    "A = chi2.ppf(0.05/2, df=compute_grad_num-1)\n",
    "B = chi2.ppf(1-0.05/2, df=compute_grad_num-1)\n",
    "var_high = (compute_grad_num-1) / A * grad_var_dpp\n",
    "var_low = (compute_grad_num-1) / B * grad_var_dpp\n",
    "xmle = np.reshape(np.log(batch_size_list),(len(batch_size_list),1)) \n",
    "ymle = np.reshape(np.log(grad_var_dpp),(len(batch_size_list),1)) \n",
    "xmle = np.hstack((xmle,np.ones((len(batch_size_list),1))))\n",
    "mle_para = np.reshape(np.dot(np.linalg.inv(np.dot(xmle.T, xmle)), np.dot(xmle.T, ymle)),(2,))\n",
    "mle_y = np.exp(mle_para[0]*np.log(batch_size_list)+mle_para[1])\n",
    "mle_slope = mle_para[0]\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots() \n",
    "plt.loglog(batch_size_list,mle_y,'g-',label='MLE slope=%.2f' %mle_slope)\n",
    "plt.loglog(batch_size_list,grad_var_iid, 'o', markeredgecolor='k', markerfacecolor='w', markersize=5)\n",
    "plt.loglog(batch_size_list,grad_var_dpp,'ko', markersize=3)\n",
    "plt.loglog(batch_size_list,var_high,'ro', markersize=2)\n",
    "plt.loglog(batch_size_list,var_low,'bo', markersize=2)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('variance')\n",
    "plt.legend(fontsize=18,loc='best')\n",
    "plt.show()\n",
    "\n",
    "A = chi2.ppf(0.05/2, df=compute_grad_num-1)\n",
    "B = chi2.ppf(1-0.05/2, df=compute_grad_num-1)\n",
    "var_high = (compute_grad_num-1) / A * grad_var_dpp\n",
    "var_low = (compute_grad_num-1) / B * grad_var_dpp\n",
    "xmle = np.reshape(np.log(batch_size_list),(len(batch_size_list),1)) \n",
    "ymle = np.reshape(np.log(grad_var_dpp),(len(batch_size_list),1)) \n",
    "xmle = np.hstack((xmle,np.ones((len(batch_size_list),1))))\n",
    "xmle = xmle[1:len(batch_size_list),:]\n",
    "ymle = ymle[1:len(batch_size_list),:]\n",
    "mle_para = np.reshape(np.dot(np.linalg.inv(np.dot(xmle.T, xmle)), np.dot(xmle.T, ymle)),(2,))\n",
    "mle_y = np.exp(mle_para[0]*np.log(batch_size_list[1:len(batch_size_list)])+mle_para[1])\n",
    "mle_slope = mle_para[0]\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots() \n",
    "plt.loglog(batch_size_list[1:len(batch_size_list)],mle_y,'g-',label='MLE slope=%.2f' %mle_slope)\n",
    "plt.loglog(batch_size_list,grad_var_iid, 'o', markeredgecolor='k', markerfacecolor='w', markersize=5)\n",
    "plt.loglog(batch_size_list,grad_var_dpp,'ko', markersize=3)\n",
    "plt.loglog(batch_size_list,var_high,'ro', markersize=2)\n",
    "plt.loglog(batch_size_list,var_low,'bo', markersize=2)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('variance')\n",
    "plt.legend(fontsize=18,loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
